\documentclass{article}

\usepackage{setspace}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{float}


\title{Time Series Analysis and Forecasting}
\author{Matthew Juan z5259434}
\date{}
\begin{document}
  \maketitle

  \newpage
  \doublespacing
  \tableofcontents
  \singlespacing
  \newpage
  \section{Introduction to Time Series'}
  Time series are found all over the place and are extremely useful
  to analyse and more importantly, forecast. There are many situations where
  you want to predict past your last data point.
  Use cases include:
  \begin{itemize}
    \item Traders predicting the stock market so they know when to buy or sell
    \item Data centre operators identifying when a critical system may go down or is acting irregularly
    \item Retail store managers knowing how to maximise what stock they order to maximise profit and minimise items that won't sell
  \end{itemize}
  Unfortunatly, just like humans, it is impossible to predict the future.
  However, by analysing the time series, it is possible to formulate
  statistical properites that can be used to give a rough estimate of
  where the data is heading.

  \section{Properties of Time Series Data}
  There are 3 components which make up a time series, trend, seasonality, and residual. Understanding their role and what they represent as well as some other key definitions are needed before diving in to analysing time series data.
  
  \subsection{Trend}
  The first component is trend. Trend can be viewed as how the data is changing overall. Is the data increasing or decreasing.
  
  \subsection{Seasonality}
  Seasonality refers to a regular occuring pattern that emerges within a given period. E.g peaks or valleys in the data during certain months or hours in the day.
  
  \subsection{Residual}
  Residual are the parts left over after fitting a model to the data. In many time series models, it is defined as the difference between the actual observation and the predicted value.
  
  \subsection{Additive vs Multiplicative}
  The trend and seasonality in a time series can be classified as being additive or multiplicative.
  In additive time series', the data exhibits a general upwards or downwards trend but does so at a constant rate. Peaks and valleys in your data are roughly the same size.
  In contrast, multiplicative time series data exhibits peaks or valleys that are amplified within the seasonal activity. The data becomes exaggerated and the difference between peaks at the start are very different than at the tail.
  
  \subsection{Stationarity}
  For time series analysis to be most effective, the data needs to form a stationary process. 
  A random variable that is a time series is classified stationary if it's probabilty distribution is constant throughout the full range of times. 
  Formally, if $T$ is the set of all times in your time series
  \begin{equation*}
    F_{X(t)}(x) = F_{X(t+\Delta)}(x)\ \forall t, t+\Delta \in T
  \end{equation*}
  Intuitively, this means a stationary time series has constant mean, variance, covariance, etc. Below shows the differences between stationary and non-stationary processes.
  \begin{figure}[H]
    \includegraphics[width=\linewidth]{stationary_time_series.png}
  \end{figure}
  \begin{flushleft}
    Stationarity can be tested using the Dickey-Fuller test which will be shown later in the implementation.

    If you have a non-stationary time series, there are a few ways to make it stationary. 
    
    The most common is differencing. Differencing involves creating a new dataset where the values at time $t$ is equal to the difference between the original value at time $t$ and original value at $t-1$. This process can be repeated multiple times to obtain a potentially more stationary series known as higher order differencing.

    Another common technique is to detrend by model fitting. You simply fit a regression model, such as linear or exponential, and create a new dataset where each value at time $t$ is equal to the difference between the predicted value and the original value.

    Other techniques include seasonal differencing, where you perform differencing but with value at time $t$ and $t-\rho$ where $\rho$ is the period of your data, or performing non-linear transformation your dataset such as logging.
  \end{flushleft}
  \newpage
  \section{Time Series Forecasting Models}
  There are many types of forecasting models but this paper will look arguably the most popular two. ARIMA (Auto-Regressive Integrated Moving Average) and Exponential Smoothing models. 
  \subsection{ARIMA}
  ARIMA is an extension of the ARMA or Box-Jenkin model popularised by George E. P. Box and Gwilym Jenkins in 1970. The main difference is that ARIMA simply allows for an extra parameter $d$ which defines how much differencing should be done before fitting the model, whereas ARMA assumes the data is stationary already. 
  As previously mentioned, a random variable in the form of a time series can be viewed as being a combination of signal and noise. The goal of ARIMA is to filter the noise and extrapolate the signal.
  \subsubsection{Formulas}
  ARIMA is a combination of an autoregressive\footnote{autoregressive = use previous values to infer future values} (AR) component and moving averages (MA)
  \subsubsection{Seasonal Variation}
  \subsection{Exponential Smoothing}
  \subsubsection{Formulas}
  \subsubsection{Seasonal Variation}

  \section{Implementation}
  \subsection{ARIMA}
  \subsection{Exponential Smoothing}

  \section{Results}

  \section{Conclusion}

  \section{Resources}
  \url{https://www.probabilitycourse.com/chapter10/10_1_4_stationary_processes.php}

  \newpage

\end{document}