\documentclass{article}

\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{float}
\usepackage{dirtytalk}

\usepackage[
backend=biber,
style=numeric,
sorting=none
]{biblatex}
\addbibresource{mybib.bib}

\title{Time Series Analysis and Forecasting}
\author{Matthew Juan z5259434}
\date{}
\begin{document}
  \maketitle

  \newpage
  \doublespacing
  \tableofcontents
  \singlespacing
  \newpage
  \section{Introduction to Time Series'}
  Time series are found all over the place and are extremely useful
  to analyse and more importantly, forecast. There are many situations where
  you want to predict past your last data point.
  Use cases include:
  \begin{itemize}
    \item Traders predicting the stock market so they know when to buy or sell
    \item Data centre operators identifying when a critical system may go down or is acting irregularly
    \item Retail store managers knowing how to maximise what stock they order to maximise profit and minimise items that won't sell
  \end{itemize}
  Unfortunatly, just like humans, it is impossible to predict the future.
  However, by analysing the time series, it is possible to formulate
  statistical properites that can be used to give a rough estimate of
  where the data is heading.
  \newpage
  \section{Properties of Time Series Data}
  A time series can fundamentally be thought of as a random variable consisting of a signal and noise. This signal and noise are formed by 3 components which are core to understanding time series', trend, seasonality, and residual. Understanding their role and what they represent as well as some other key definitions are needed before diving in to analysing time series data.
  
  \subsection{Trend}
  The first component is trend. Trend can be viewed as how the data is changing overall. Is the data increasing or decreasing.
  
  \subsection{Seasonality}
  Seasonality refers to a regular occuring pattern that emerges within a given period. E.g peaks or valleys in the data during certain months or hours in the day.
  
  \subsection{Residual}
  Residual, also commonly referred to as errors, are the parts left over after fitting a model to the data. In many time series models, it is defined as the difference between the actual observation and the predicted value.
  
  \subsection{Additive vs Multiplicative}
  The trend and seasonality in a time series can be classified as being additive or multiplicative.
  In additive time series', the data exhibits a general upwards or downwards trend but does so at a constant rate. Peaks and valleys in your data are roughly the same size.
  In contrast, multiplicative time series data exhibits peaks or valleys that are amplified within the seasonal activity. The data becomes exaggerated and the difference between peaks at the start are very different than at the tail.
  \newpage
  \subsection{Stationarity}
  Before going into analysis, the time series needs to be stationary.
  A random variable that forms a time series is classified stationary if it's probabilty distribution is constant throughout the full range of times. 
  Formally, if $T$ is the set of all times in your time series of length $n$, a random process $X$ is stationary if the joint probability distribution at times $t_1,...,t_n \in T$ and $t_1+\Delta,...,t_n+\Delta \in T,\  \Delta \in \mathbb{R}$ are equal. That is
  \begin{equation*}
    F_{X}(x_{t_1},...,x_{t_n}) = F_{X}(x_{{t_1}+\Delta},...,x_{{t_n}+\Delta})\ \forall x_{t_1},...,x_{t_n} \in T
  \end{equation*}
  Intuitively, this means a stationary time series has constant propertues such as mean, variance, covariance, etc. Below shows the differences between stationary and non-stationary processes.
  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{stationary_time_series.png}
  \end{figure}

  \subsubsection{How to test for stationarity}
    To determine whether a time series is stationary or not, the Augmented Dickey Fuller(ADF) test can be performed. ADF performs a hypothesis test and upon observing p-value $p < \alpha$ where $\alpha$ is commonly 0.05, the hypothesis is rejected and the data is considered to be stationary.

  \subsubsection{Methods of making a time series stationary}
    If you have a non-stationary time series, there are a few ways to make it stationary. Let's use the Shampoo Sales dataset which describes the monthly number of sales of shampoo over a 3 year period.
    The data here has a distinct multiplicative increasing trend and is clearly non stationary. Using the ADF test, the p-value comes out to $1>0.05$ so there is weak evidence against the null hypothesis so the data is non-stationary.
    \begin{figure}[H]
      \includegraphics[width=\linewidth]{data_both.png}
    \end{figure}
    The most common is differencing. Differencing involves creating a new dataset where the values at time $t$ is equal to the difference between the original value at time $t$ and original value at $t-1$. So if $y$ is the new dataset and $Y$ is the original data, then
    \begin{equation*}
      y_t = Y_t - Y_{t-1}.
    \end{equation*}
    Sometimes the differenced data will not make the data stationary enough and it may be necessary to difference the data again known as 2nd order differencing. This process is applicable to higher orders
    Note, the 2nd order difference is not $y_t = Y_t - Y_{t-2}$ but it is the first differnce of the first difference so it comes out to
    \begin{equation*}
      y_t = (Y_t - Y_{t-1}) - (Y_{t-1} - Y_{t-2}).
    \end{equation*}
    The p-value from the ADF test is about $1.8 \times 10^{-10} < 0.05$ so the data is now stationary.
    \begin{figure}[H]
      \centering
      \includegraphics[width=\linewidth]{diff_both.png}
    \end{figure}
    Another common technique is to detrend by model fitting. You simply fit a regression model, such as linear or exponential, and create a new dataset where each value at time $t$ is equal to the difference between the original value and the predicted value.
    When fitted with a 2nd order polynomial regression model, the resulting differences produced a p-value of about $3.4 \times 10^{-10}< 0.05$ so again, the data is now stationary.
    \begin{equation*}
      y_t = Y_t - pred_t.
    \end{equation*}
    Again, the data is much more stationary than the original.
    \begin{figure}[H]
      \centering
      \includegraphics[width=\linewidth]{reg_both.png}
    \end{figure}
    There are various other techniques that can be performed and combined with one another to make your data more stationary.
    If your data has a seasonal pattern to it, seasonal differencing can be performed where you take the difference between $t$ and $t-\rho$ where $\rho$ is the period of your data.
    Non-linear transformations your dataset such as logging can also prove effective at making data stationary.
  \newpage
  \section{Time Series Forecasting Models}
    There are many types of forecasting models but this paper will look arguably the most popular two. ARIMA (Auto-Regressive Integrated Moving Average) and Exponential Smoothing models. 
  \subsection{ARIMA}
    ARIMA is an extension of the ARMA or Box-Jenkin model popularised by George E. P. Box and Gwilym Jenkins in 1970. The main difference is that ARIMA simply allows for an extra parameter $d$ which defines how much differencing should be done before fitting the model, whereas ARMA assumes the data is stationary already. 
    ARIMA is actually a combination of two models, an autoregressive model (AR) and a moving averages model (MA). We will dive into what makes up both models and how ARIMA is able to effectively combine the two to create an even stronger model.
    As previously mentioned, a random variable in the form of a time series can be viewed as being a combination of signal and noise. The goal of ARIMA is to filter the noise and extrapolate the signal.
  \subsubsection{Autoregressive Models}
    The term \emph{autoregressive} defines that the model uses a linear combination of previous values of a variable and learned predictors. This differs from a typical regression model which predicts based on a linear combination of the input features and learned predictors.
    Autoregressive models has one parameter $p$ which defines the order of the model. This parameter defines how many previous values influence the one being predicted. 
    An $AR(1)$ model is simply defines a model in which the predicted value is equal to some linear combination of the value at the previous time step
    \begin{equation*}
      y_t = \mu + \phi_1 y_{t-1} + \varepsilon_t,
    \end{equation*}
    where $\mu$ is the average period to period change and $\varepsilon_t$ is some white noise.
    An $AR(2)$ model uses the previous two timesteps as so on.
    Thus, an $AR(p)$ model can be generalised to
    \begin{equation*}
      y_t = \mu + \phi_1 y_{t-1} + \phi_2 y_{t-2} + ... + \phi_p y_{t-p} + \varepsilon_t.
    \end{equation*}
    We can see that we are using previous known values as the features to predict the future.

    Let's see how this works for the first differenced data we saw before. For my implementation, to estimate the parameters I used a linear least squares solver. Firstly, the lags are calculated and then we estimate parameters using these lags and our original time series values. We simply do a dot product between the coefficients and add it to our value for $\mu$ to retrieve our predictions. 
    
    This is the resulting predictions of a few $AR(p)$ models.

    There isn't too much change apart from the beginning of the data series where no lags could be properly calculated so they were filled with 0.

    The already known lags are used to forecast one time step ahead. This forecast is then assumed to be the observed value and further values can be forecasted in a similar manner.

  \subsubsection{Moving Average Models}
    In a sort of similar manner, Moving Average (MA) models use the errors of past forecasts to predict. An MA model instead has parameter $q$ so an $MA(q)$ model can be written similarly as
    \begin{equation*}
      y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + ... + \theta_q \varepsilon_{t-q}.
    \end{equation*}
    If your data is properly stationary, the error terms produced, $\varepsilon_t$, should form white noise and form a normal distribution with $0$ mean and $\sigma^2$ variance.

    Parameters were again estimated using a linear least squares solver and this is the results of a few $MA(q)$ models on the same data. Observe that the residuals produced do generally form a normal distribution with 0 mean.

  \subsubsection{Putting It All Together}
  When we combine differencing, an AR, and an MA model, we obtain an ARIMA model. It's general equation is as follows
  \begin{equation*}
    y_t' = \mu + \phi_1 y_{t-1}' + ... + \phi_p y_{t-p}' + \theta_1 \varepsilon_{t-1}' + ... + \theta_q \varepsilon_{t-q}' + \varepsilon_t
  \end{equation*}
  where $y_t'$ is the differenced data.

  An ARIMA model has 3 parameters and can fully be expressed as ARIMA$(p,d,q)$.
  \begin{itemize}
    \item $p$ is order of autoregressive component
    \item $d$ is the order of differencing to be performed on the data
    \item $q$ is the order of moving averages component
  \end{itemize}
  This generalised model is great since we don't have to difference the data ourselves and allows for the combination of $AR(p)$ and $MA(q)$ models. The implementation of this is quite simple as you just have to add the results of the AR and MA components together.

  Running through a list of parameters, we can see effects of different values for the parameters.

  \subsubsection{Model Scoring}
  There are a few ways to score our model to determine how well it performs on unseen data. The way we will score our models is using Akaine's Information Criteria. It can be written as 

  \begin{equation*}
    AIC = -2ln(L) + 2(p + q + k + 1),
  \end{equation*}
  where $L$ is the likelihood of the data, $k = 1$ if $\mu \ne 0$ and $k = 0$ if $\mu = 0$. In our case, $L$ will be defined as the sum of squares of the errors, namely,

  \begin{equation*}
    L = \sum_{t=1}^{T} \varepsilon_t^2.
  \end{equation*}

  \subsubsection{Order Choosing}
  Up until now, we have been manually setting the parameters $p$, $d$, and $q$ seemingly by observing which model fits the best. However, there are systematic methods of narrowing down the search space. This is where we look at the autocorrelation function (ACF) plots and partial autocorrelation (PACF) plots to help determine our optimal parameters. Autocorrelation is a way of measuring the similarity between values in a time series and their past. An autocorrelation of $+1+$ means that there is a perfect positive correlation while $-1$ represents a perfect negative correlation. Partial autocorrelation is similar to autocorrelation but slightly more complicated to compute. PACF again measures similarity between $y_t$ and $y_{t-\Delta}$ but adjusts for the presence of the other terms of short lag ($y_{t-1}, y_{t-2},...,y_{t-\Delta-1}$).
  Here is the ACF and PACF plots of our non-differenced and once differenced data.

  In a perfect scenario, we would want no correlation between the series and the lags of itself. Of course, this is almost impossible to achieve so having the spikes fall inside the blue region is close to good enough. 

  Firstly, we have to choose the order of differencing. It is not sufficient to say that because a once differenced series is stationary then that should be the order. Higher order differencing may produce better results. This is where we look at the autocorrelation function (ACF) plot to help pick the value for $d$. 

  Duke University professor, Dr Robert Nau presents a few rules for interpreting these graphs and how to estimate the optimal value for $d$.

  \emph{\say{Rule 1: If the series has positive autocorrelations out to a high number of lags (say, 10 or more), then it probably needs a higher order of differencing.}}

  \emph{\say{Rule 2: If the lag-1 autocorrelation is zero or negative, or the autocorrelations are all small and patternless, then the series does not need a higher order of differencing. If the lag-1 autocorrelation is -0.5 or more negative, the series may be overdifferenced.  BEWARE OF OVERDIFFERENCING.}}

  Looking at the ACF plot for our original data, it mostly stays positive until around lag 11 so by rule 1, we should difference at least once.

  Here, we can see that rule 1 no longer applies and rule 2 is applied. Observe that the lag-1 autocorrelation does go negative so a good choice would be $d=1$. Note that the value is less than $-0.5$ which the rule warns us of overdifferencing so we will compare the results of an $ARIMA(0,0,0)$ model to an $ARIMA(0,1,0)$ to see which one performs better. 

  Clearly, $ARIMA(0,1,0)$ is the better performer so $d=1$ is a good choice.
  
  By looking at the ACF and PACF plots of a stationary series, we can generally identify the numbers of AR and/or MA terms that are needed. Dr Nau has identified more rules in identifying these values.
  
  \emph{\say{Rule 6: If the partial autocorrelation function (PACF) of the differenced series displays a sharp cutoff and/or the lag-1 autocorrelation is positive, then consider adding one or more AR terms to the model. The lag which the PACF cuts off is the indicated number of AR terms.}}

  \emph{\say{Rule 7: If the autocorrelation function (ACF) of the differenced series displays a sharp cutoff and/or the lag-1 autocorrelation is negative, then consider adding an MA term to the model. The lag which the ACF cuts off is the indicated number of MA terms.}}

  Rule 6 helps us determine the AR terms. From the PACF plot, at lag-1 there is a steep drop into the negatives then settles into the blue region. Hence, we probably need 1 AR term. Similarly by rule 7, we should pick 1 MA term.

  This gives us an $ARIMA(1,1,1)$ model and this is how it fits with the data.

  Running this through a parameter optimiser, it gives back a parameter list of $(1,1,2)$ so our estimation was pretty close and shows that these rules are not absolute.
  
  \subsubsection{Seasonal Variation}
  Many time series data in the real work exhibit some form of seasonality. These could include higher network traffic during peak hours of the day or increased spending during Christmas. In such cases, we need to take differences greater than 1 known as the seasonal difference. For example, if there are 12 periods in a season, the seasonal diffence is $y_t = Y_t - Y_{t-12}$. Taking a seasonal difference can again help with making the data more stationary.

  A seasonal ARIMA model has two components, the non-seasonal and seasonal, where they are both structurally the same. Each can have their own AR, MA and order of differencing parameters. Seasonal ARIMA models can be written as 
  \begin{equation*}
    ARIMA(p,d,q)\times(P,D,Q)_m
  \end{equation*}
  where $P$, $D$, and $Q$ are the seasonal AR (SAR) terms, seasonal differencing, and seasonal MA (SMA) terms respectively with $m$ being the period.

  Let's use the monthly milk dataset as an example.

  There is a clear rise and fall each year and the total increases each year. Since our data is in months, we will use $m = 12$.

  Once again, Dr Nau has 2 rules in picking these parameters:
  
  \emph{\say{Rule 12: If the series has a strong and consistent seasonal pattern, then you should use an order of seasonal differencing--but never use more than one order of seasonal differencing or more than 2 orders of total differencing (seasonal+nonseasonal).}}

  \emph{\say{Rule 13: If the autocorrelation at the seasonal period is positive, consider adding an SAR term to the model. If the autocorrelation at the seasonal period is negative, consider adding an SMA term to the model. Try to avoid mixing SAR and SMA terms in the same model, and avoid using more than one of either kind.}}

  \subsection{Exponential Smoothing}
  \subsubsection{Formulas}
  \subsubsection{Seasonal Variation}

  \section{Implementation}
  \subsection{ARIMA}
  \subsection{Exponential Smoothing}

  \section{Results}

  \section{Conclusion}

  \section{Resources}
  \url{https://www.probabilitycourse.com/chapter10/10_1_4_stationary_processes.php}
  \url{https://www.statology.org/detrend-data/}

  bibliography\cite{articleFactCheck}
  \printbibliography

\end{document}