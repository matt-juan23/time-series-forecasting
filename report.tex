\documentclass{article}

\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{float}
\usepackage{dirtytalk}

\usepackage[
backend=biber,
style=numeric,
sorting=none
]{biblatex}
\addbibresource{mybib.bib}

\title{Time Series Analysis and Forecasting}
\author{Matthew Juan z5259434}
\date{}
\begin{document}
  \maketitle
  % \begin{flushleft}
  \newpage
  \doublespacing
  \tableofcontents
  \singlespacing
  \newpage
  \section{Introduction to Time Series'}
  Time series are found all over the place and are extremely useful
  to analyse and more importantly, forecast. There are many situations where
  you want to predict past your last data point.
  Use cases include:
  \begin{itemize}
    \item Traders predicting the stock market so they know when to buy or sell
    \item Data centre operators identifying when a critical system may go down or is acting irregularly
    \item Retail store managers knowing how to maximise what stock they order to maximise profit and minimise items that won't sell
  \end{itemize}
  Unfortunatly, just like humans, it is impossible to predict the future.
  However, by analysing the time series, it is possible to formulate
  statistical properites that can be used to give a rough estimate of
  where the data is heading.
  
  \newpage
  
  \section{Non-Seasonal Forecasting Models}
    There are many types of forecasting models but this paper will look two types. Exponential Smoothing and ARIMA (Auto-Regressive Integrated Moving Average) models. For these non-seasonal models, we will use the Shampoo Sales dataset since there is no clear seasonality to the data.
    \begin{figure}[H]
      \includegraphics[width=\linewidth]{data_both.png}
    \end{figure}

  \subsection{Exponential Smoothing}
    As the name suggests, Exponential Smoothing models are essentially weighted averages of past observations which decay exponentially over time. This type allows for a reliable and quick way to forecast for a wide array of time series'. This section of the paper will go into an implementation of simple exponential smoothing, Holt's linear trend method, and the damped trend method.
  

  \subsubsection{Simple Exponential Smoothing}
  This method of forecasting is extremely simple and is useful when there is no clear trend or seasonal pattern in the data. As mentioned, this method uses a weighted average where the weights decrease exponentially the further in the past the associated observation is. Therefore, for a time series with $T$ data points, we can define such an forecast equation as:
  \begin{equation*}
    \hat{y}_{T+1} = \alpha y_T + \alpha(1-\alpha)y_{T-1} + \alpha(1-\alpha)^2y_{T-2} + \alpha(1-\alpha)^3y_{T-3} + ...,
  \end{equation*}
  where $0 \le \alpha \le 1$ is the smoothing parameter and $\hat{y}_{T}$ is the forecasted value at time $T+1$. This equation can be simplified by using the forecasted value at time $T$ to predict the value at time $T+1$ resulting in
  \begin{equation*}
    \hat{y}_{T+1} = \alpha y_T + (1-\alpha)\hat{y}_{T}.
  \end{equation*}

  This equation can also be expressed in component form defined as a level $\ell_t$ smoothing equation and forecasting equation:
  \begin{align*}
    & \text{Level equation} & \ell_t &= \alpha y_t + (1-\alpha)(\ell_{t-1})  \\
    & \text{Forecast equation} & \hat{y}_{t+h} &= \ell_t \\
  \end{align*}
  where $0 \le \alpha \le 1$. The forecasting equation produces a "flat" forecast equal to the last level component since setting $t=T$, we get
  \begin{equation*}
    \hat{y}_{T+h} = \ell_T, \qquad h=1,2,....
  \end{equation*}
  
  Let's see how this is applied to the shampoo dataset. Firstly, we need to estimate values for $\alpha$ as well as the initial level $\ell_0$. By minimising the SSE (sum of squared errors), the estimated parameters are $\alpha=0.40$ and $\ell_0=202.78$. The table below shows the calculations using these parameters.

  \begin{center}
    \begin{tabular}{||c c c c||} 
     \hline
     Time($t$) & Observation($y_t$) & Level($\ell_t$) & Forecast($\hat{y}_t$) \\ [0.5ex] 
     \hline
     0 &       & 202.78 &  \\ 
     \hline
     1 & 266.0 & 228.03 & 202.78 \\
     \hline
     2 & 145.9 & 195.23 & 228.03 \\
     \hline
     3 & 183.1 & 190.38 & 195.23 \\
     \hline
     4 & 119.3 & 161.99 & 190.38 \\
     \hline
     \vdots & \vdots & \vdots & \vdots \\
     \hline
     33 & 682.0 & 541.92 & 448.78 \\
     \hline
     34 & 475.3 & 515.32 & 541.92 \\
     \hline
     35 & 581.3 & 541.67 & 515.32 \\
     \hline
     36 & 646.9 & 583.70 & 541.67 \\
     \hline
     $h$ &  &  & $\hat{y}_{T+h}$ \\
     \hline
     1 &  &  & 583.70 \\
     \hline
     2 &  &  & 583.70 \\
     \hline
     3 &  &  & 583.70 \\
     \hline
     4 &  &  & 583.70 \\
     \hline
     5 &  &  & 583.70 \\
     \hline
    \end{tabular}
  \end{center}

  \begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{simple_exp_fit.png}
      \caption{Fitted values}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{simple_exp_forecast.png}
      \caption{Forecasted values}
    \end{subfigure}
    \caption{Fitted and forecasted values using simple exponential smoothing}
  \end{figure}

  \subsubsection{Trend Methods}
  This section will show Holt's linear trend method and Gardner \& McKenzie's damped trend method which extend simple exponential smoothing by incorporating the trend component. Trend is simply the general upwards or downwards motion of the data.

  \subsubsection*{Holt's Linear Trend Method}
  This method now has three equations, a level smoothing equation, a trend smoothing equation, and a forecast equation:
  \begin{align*}
    \text{Forecast equation} && \hat{y}_{t+h|t} &= \ell_{t} + hb_{t} \\
    \text{Level equation}   && \ell_{t} &= \alpha y_{t} + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
    \text{Trend equation}   && b_{t}    &= \beta(\ell_{t} - \ell_{t-1}) + (1 -\beta)b_{t-1},
  \end{align*}
  where $0 \le \alpha, \beta \le 1$. By including the trend, forecasts are no longer flat but a linear function of $h$. As such, the forecasted value at time $T+h$ is equal to the last level plus $h$ multiplied by the last calculated trend value.

  There are now two smoothing parameters $\alpha$ and $\beta$ as well as two initial states $\ell_0$ and $b_0$ which are estimated to $\alpha=0.49, \beta=0.27, \ell_0=262.93, b_0=-18.01$. Giving an initial trend of $y_1-y_0$ can help the optimizer narrow the search space for this parameter.

  \begin{center}
    \begin{tabular}{||c c c c c||} 
     \hline
     Time($t$) & Observation($y_t$) & Level($\ell_t$) & Slope($b_t$) & Forecast($\hat{y}_t$) \\ [0.5ex] 
     \hline
     0 &       & 262.93 & -18.01 &  \\ 
     \hline
     1 & 266.0 & 255.19 & -15.23 & 244.92 \\
     \hline
     2 & 145.9 & 194.14 & -27.65 & 244.92 \\
     \hline
     3 & 183.1 & 174.58 & -25.46 & 239.96 \\
     \hline
     4 & 119.3 & 134.60 & -29.40 & 166.489 \\
     \hline
     \vdots & \vdots & \vdots & \vdots & \vdots \\
     \hline
     33 & 682.0 & 583.60 & 39.51 & 540.92 \\
     \hline
     34 & 475.3 & 551.11 & 19.99 & 490.15 \\
     \hline
     35 & 581.3 & 576.07 & 21.33 & 623.11 \\
     \hline
     36 & 646.9 & 621.51 & 27.87 & 571.10 \\
     \hline
     $h$ &  &  &  & $\hat{y}_{T+h}$ \\
     \hline
     1 &  &  &  & 649.38 \\
     \hline
     2 &  &  &  & 677.25 \\
     \hline
     3 &  &  &  & 705.12 \\
     \hline
     4 &  &  &  & 732.99 \\
     \hline
     5 &  &  &  & 760.86 \\
     \hline
    \end{tabular}
  \end{center}

  \begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{holt_linear_fit.png}
      \caption{Fitted values}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{holt_linear_forecast.png}
      \caption{Forecasted values $\phi=0.9$}
    \end{subfigure}
    \caption{Fitted and forecasted values using Holt linear trend method}
  \end{figure}

  \subsubsection*{Damped Trend Method}
  Holt's Linear method show a constant increasing or decreasing trend which extends indefinitely into the future. However, in reality this method can be quite inaccurate for extended forecasts. Gardner \& McKenzie introduced a dampening parameter $\phi$, where $0 \le \phi \le 1$. This parameter dampens the trend to forecast a flat line after some time.

  The component form of this method is:
  \begin{align*}
    \text{Forecast equation}&& \hat{y}_{t+h|t} &= \ell_{t} + (\phi+\phi^2+...+\phi^h)b_{t} \\
    \text{Level equation}   && \ell_{t} &= \alpha y_{t} + (1 - \alpha)(\ell_{t-1} + \phi b_{t-1})\\
    \text{Trend equation}   && b_{t}    &= \beta(\ell_{t} - \ell_{t-1}) + (1 -\beta)\phi b_{t-1}.
  \end{align*}
  When $\phi=1$, this method produces Holt's linear method.

  \begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=\linewidth]{damped_forecast.png}
    \caption{Forecasts for different values of $\phi$}
  \end{figure}

  \subsection{ARIMA}
    ARIMA is an extension of the ARMA or Box-Jenkin model popularised by George E. P. Box and Gwilym Jenkins in 1970. The main difference is that ARIMA simply allows for an extra parameter $d$ which defines how much differencing should be done before fitting the model, whereas ARMA assumes the data is stationary already. 
    ARIMA is actually a combination of three components, an autoregressive model (AR), a moving averages model (MA), and an integrated (I) part denoting the order of differencing. The ultimate goal of ARIMA is to filter the noise and extrapolate the signal wihtin a time series.
  
  \subsubsection{Stationarity}
    ARIMA models work best on data that is "stationary". Intuitively, a stationary time series has statistical properties, such as mean, variance, covariance, etc, that remain constant throughout the whole time.
    Below shows the differences between stationary and non-stationary processes.
    \begin{figure}[H]
      \centering
      \includegraphics[scale=0.8]{stationary_time_series.png}
    \end{figure}

  \subsubsection{How to test for stationarity}
    To determine whether a time series is stationary or not, the Augmented Dickey Fuller(ADF) test can be performed. ADF performs a hypothesis test and upon observing p-value $p < \alpha$ where $\alpha$ is commonly 0.05, the hypothesis is rejected and the data is considered to be stationary.

  \subsubsection{Methods of making a time series stationary}
    If you have a non-stationary time series, there are a few ways to make it stationary. The shampoo dataset is clearly non stationary because there is a defined upwards trend. Using the ADF test, the p-value comes out to $1>0.05$ so there is weak evidence against the null hypothesis which confirms our statement.

    The most common practice to make a time series stationary is differencing. Differencing involves creating a new dataset where the values at time $t$ is equal to the difference between the original value at time $t$ and original value at $t-1$. So if $Y$ is the new dataset and $y$ is the original data, then
    \begin{equation*}
      Y_t = y_t - y_{t-1}.
    \end{equation*}
    The p-value from the ADF test is about $1.8 \times 10^{-10} < 0.05$ so the data is now stationary when differenced once.
    \begin{figure}[H]
      \centering
      \includegraphics[width=\linewidth]{diff_both.png}
    \end{figure}
    Sometimes the differenced data will not make the data stationary enough and it may be necessary to difference the data again known as 2nd order differencing. Note, the 2nd order difference is the first differnce of the first difference so it comes out to
    \begin{equation*}
      Y_t = (y_t - y_{t-1}) - (y_{t-1} - y_{t-2}).
    \end{equation*}
    There are various other techniques that can be performed and combined with one another to make your data more stationary. These include model fitting, non-linear transformations, or seasonal differencing.

    \subsubsection{Backshift Notation}
    Before we jump into defining what an ARIMA model is, there is a useful backshift operator $B$ denoting the time series lags,
    \begin{equation*}
      By_t = y_{t-1}.
    \end{equation*}
    $B$ effectively shifts that data back one time period. So naturally, two applications of $B$ shifts the data back two time periods. 
    \begin{align*}
      B(By_t) &= B^2y_t \\
      &= y_{t-2}.
    \end{align*}
    If we describe how differencing works using this notation it comes out to
    \begin{align*}
      Y_t &= y_t - y_{t-1} \\
      &= y_t - By_t \\
      &= (1 - B)y_t.
    \end{align*}
    In general, the $d$'th order of differencing is defined as 
    \begin{equation*}
      Y_t = (1 - B)^dy_t.
    \end{equation*}
    This notation is great as we can use $B$ is an algebraic manner which greatly simplifies some upcoming formulas.

  \subsubsection{Autoregressive Models}
    The term \emph{autoregressive} defines that the model uses a linear combination of previous values of a variable and learned predictors. This differs from a typical regression model which predicts based on a linear combination of the input features and learned predictors.
    Autoregressive models has one parameter $p$ which defines the order of the model. This parameter defines how many previous values influence the one being predicted. 
    An AR$(1)$ model is simply defines a model in which the predicted value is equal to some linear combination of the value at the previous time step
    \begin{equation*}
      y_t = \mu + \phi_1 y_{t-1} + \varepsilon_t,
    \end{equation*}
    where $\mu$ is the average period to period change.
    An AR$(2)$ model uses the previous two timesteps as so on.
    Thus, an AR$(p)$ model can be generalised to
    \begin{gather*}
      y_t = \mu + \phi_1 y_{t-1} + \phi_2 y_{t-2} + ... + \phi_p y_{t-p}\\
      \text{or in equivalent backshift notation}\\
      (1 - \phi_1B - \phi_2B^2 - ... - \phi_pB^p)y_t = \mu.
    \end{gather*}
    We can see that we are using previous known values as the features to predict the future.

    Let's see how this works for the first differenced data. Firstly, the lags are calculated and then we estimate parameters using these lags and our original time series values. We simply do a dot product between the coefficients and add it to our value for $\mu$ to retrieve our predictions. 
    
    This is the resulting predictions of a few AR$(p)$ models.

    \begin{figure}[H]
      \centering
      \captionsetup{justification=centering}
      \begin{subfigure}[b]{0.49\linewidth}
        \includegraphics[width=\linewidth]{AR_1.png}
        \caption{AR(1)}
      \end{subfigure}
      \begin{subfigure}[b]{0.49\linewidth}
        \includegraphics[width=\linewidth]{AR_2.png}
        \caption{AR(2)}
      \end{subfigure}
      \caption{Two types of AR models. AR(1) with estimated $\phi_1 = -0.70868438$. AR(2) with estimated $\phi_1 = -0.9237595, \phi_2 =   -0.26627682$.}
    \end{figure}

    Observe that there are subtle changes between the two. If we kept on increasing the AR term, our model would fit this data extremely well. In machine learning, this is commonly known as overfitting and future forecasts can be very inaccurate.

  \subsubsection{Moving Average Models}
    In a sort of similar manner, Moving Average (MA) models use the errors of past forecasts to predict. An MA model instead has parameter $q$ so an MA$(q)$ model can be written similarly as
    \begin{gather*}
      y_t = \mu + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + ... + \theta_q \varepsilon_{t-q}\\
      \text{or in equivalent backshift notation}\\
      y_t = \mu + (\theta_1B + \theta_2B^2 + ... + \theta_qB^q)\varepsilon_t.
    \end{gather*}
    If your data is properly stationary, the error terms produced should form white noise in the form of a normal distribution with $0$ mean and $\theta^2$ variance.

    \begin{figure}[H]
      \centering
      \captionsetup{justification=centering}
      \begin{subfigure}[b]{0.49\linewidth}
        \includegraphics[width=\linewidth]{residuals.png}
        \caption{MA(1)}
      \end{subfigure}
      \begin{subfigure}[b]{0.49\linewidth}
        \includegraphics[width=\linewidth]{residuals2.png}
        \caption{MA(2)}
      \end{subfigure}
      \caption{Error terms for MA(1) with estimated $\theta_1 = -0.58305195$ and MA(2) with estimated $\theta_1 = -0.71206191, \theta_2 =  0.28793809$. Both generally look normally distributed with 0 mean.}
    \end{figure}

  \subsubsection{Putting It All Together}
  When we combine differencing, an AR, and an MA model, we obtain an ARIMA model. An ARIMA model has 3 parameters and can be expressed as ARIMA$(p,d,q)$
  \begin{itemize}
    \item $p$ is order of autoregressive component
    \item $d$ is the order of differencing to be performed on the data
    \item $q$ is the order of moving averages component
  \end{itemize}
  It's general equation is as follows
  \begin{gather*}
    y_t' = \mu + \phi_1 y_{t-1}' + ... + \phi_p y_{t-p}' + \theta_1 \varepsilon_{t-1}' + ... + \theta_q \varepsilon_{t-q}'\\
    \text{or in equivalent backshift notation}\\
    (1 - \phi_1B - ... - \phi_pB^p)(1-B)^dy_t = \mu + (\theta_1B + ... + \theta_qB^q)\varepsilon_t.
  \end{gather*}
  where $y_t'$ is the differenced data. This generalised model is great since we don't have to difference the data ourselves and allows for the combination of AR$(p)$ and MA$(q)$ models.  

  Running through a list of parameters, we can see effects of different values for the parameters.

  \begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{ARIMA_010.png}
      \caption{ARIMA$(0, 1, 0)$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{ARIMA_111.png}
      \caption{ARIMA$(1, 1, 1)$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{ARIMA_110.png}
      \caption{ARIMA$(1, 1, 0)$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{ARIMA_011.png}
      \caption{ARIMA$(0, 1, 1)$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{ARIMA_211.png}
      \caption{ARIMA$(2, 1, 1)$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{ARIMA_112.png}
      \caption{ARIMA$(1, 1, 2)$}
    \end{subfigure}
    \caption{Fits for different values of $p$ and $q$}
  \end{figure}

  Picking the right parameter values is dataset specific but often there are auto\_arima packages in Python and R which can find optimal values based on your data. Observing the autocorrelation function (ACF) and partial autocorrelation function (PACF) plots often also give a good indication of what parameter values you should choose.

  \subsubsection{Coefficient Estimation}
  Estimating the coefficients for an ARIMA model is often a tricky task as there needs to be a balance between having a good fit for the data while not overfitting. For my implementation, both the AR and MA terms were calculated similarly using the Constrained Optimization BY Linear Approximation (COBYLA) algorithm which optimises based on the sum of squared errors. To avoid overfitting the data, I set constraints on the coefficients namely,
  \begin{itemize}
    \item $-1 \leq \phi_p \leq 1$
    \item $\phi_i + \phi_{i+1} \leq 1 \quad \text{for} \ i=1,2,...,p-1$
    \item $\phi_{i+1} - \phi_i \leq 1 \quad \text{for} \ i=1,2,...,p-1$
  \end{itemize}
  
  The same constraints are put for $\theta_1,...,\theta_q$. In practice, ARIMA packages in Python and R use Maximum Likelihood Estimation to estimate the coefficients which often invoves much more complicated computations. There are also more complicated constraints, especially as the values of $p$ and $q$ increase, which these packages handle that account for the invertability and stationarity constraints.

  \subsubsection{Forecasting}
  So far, we have shown how to fit an ARIMA model on a well-defined time series. But we have not gone into how it forecasts unseen data. It is quite simple and involves 3 steps:
  \begin{enumerate}
    \item Expand the model's ARIMA backshift equation and group by the powers of B
    \item Convert to equation with only $y_t$'s and move all terms that isn't $y_t$ to the right side of the equation
    \item Replace $t$ with $T+1$ is the number of future time steps and update time series with predicted value
  \end{enumerate}
  Let's try this with an example using an ARIMA(2,1,2) model as an example. In backshift notation, the model is written as
  \begin{equation*}
    (1 - \phi_1B - \phi_2B^2)(1-B)y_t = \mu + (\theta_1B + \theta_2B^2)\varepsilon_t.
  \end{equation*}
  Using step 1, we expand and group:
  \begin{align*}
    (1 - B - \phi_1B + \phi_1B^2 - \phi_2B^2 + \phi_2B^3)y_t &= \mu + (\theta_1B + \theta_2B^2)\varepsilon_t\\
    [1 - (1 + \phi_1)B - (\phi_2 - \phi_1)B^2 + \phi_2B^3]y_t &= \mu + (\theta_1B + \theta_2B^2)\varepsilon_t.
  \end{align*}
  With second step, we convert giving
  \begin{equation*}
    y_t - (1 + \phi_1)y_{t-1} - (\phi_2 - \phi_1)y_{t-2} + \phi_2y_{t-3} = \mu + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2},\\
  \end{equation*}
  then moving terms
  \begin{equation*}
    y_t = \mu + (1 + \phi_1)y_{t-1} + (\phi_2 - \phi_1)y_{t-2} - \phi_2y_{t-3} + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2}.
  \end{equation*}
  Third step is to replace $t$ with $T+1$:
  \begin{equation*}
    y_{T+1} = \mu + (1 + \phi_1)y_{T} + (\phi_2 - \phi_1)y_{T-1} - \phi_2y_{T-2} + \theta_1\varepsilon_{T-1} + \theta_2\varepsilon_{T-2}.
  \end{equation*}
  To predict further into the future say at time $T+2$, then just repeat step 3 which produces
  \begin{equation*}
    y_{T+2} = \mu + (1 + \phi_1)y_{T+1} + (\phi_2 - \phi_1)y_{T} - \phi_2y_{T-1} + \theta_2\varepsilon_{T-1}.
  \end{equation*}
  Observe that there should be a $\theta_1\varepsilon_{T}$ term but since we set the time series value at $T$ to be equal to the predicted value, this error term evaluates to 0. We are essentially taking our predicted value as the truth which is why all the error terms eventually become 0. Hence, if you predict $S > q$ future time steps, all MA terms disappear. 

  \begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{forecast_ARIMA_010.png}
      \caption{ARIMA$(0, 1, 0)$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{forecast_ARIMA_111.png}
      \caption{ARIMA$(1, 1, 1)$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{forecast_ARIMA_110.png}
      \caption{ARIMA$(1, 1, 0)$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{forecast_ARIMA_011.png}
      \caption{ARIMA$(0, 1, 1)$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{forecast_ARIMA_211.png}
      \caption{ARIMA$(2, 1, 1)$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{forecast_ARIMA_112.png}
      \caption{ARIMA$(1, 1, 2)$}
    \end{subfigure}
    \caption{Forecasting for 15 future time steps from different ARIMA models. (c) and (d) are equivalent to an AR(1) and MA(1) with 1 order of differencing.}
  \end{figure}
  
  \newpage

  \section{Seasonal models}
  Many time series data in the real work exhibit some form of seasonality. This includes higher network traffic during peak hours of the day or increased spending during Christmas. As such, there is a defined frequency of seasonality commonly denoted as $m$. For example $m=4$ could represent quarterly data and $m=12$ for monthly data. We will again explore the ARIMA and Exponential Smoothing algorithms and how they were extended to account for seasonality. For these examples, we will use the monthly milk dataset. This dataset is great since there is a clear seasonal pattern and the data is trending upward.
  \begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{\linewidth}
      \includegraphics[width=\linewidth]{milk.png}
    \end{subfigure}
    \caption{Milk dataset}
  \end{figure}

  \newpage
  \subsection{Holt-Winters' Seasonal Method}
  Holt and Winters improved Holt's linear method to capture seasonality. This method includes a level equation, trend equation, seasonal equation, and forecasting equation. It introduces a new smoothing parameter $\gamma$ and $m$ initial seasonal values that have to be estimated. This method comes in two flavours, an additive and multiplicative method. Which one you pick is dependent on your data. The additive method is preferred when seasonal variations and trend change at a constant rate. Peaks and valleys in your data are roughly of the same magnitude. In contrast, multiplicative is preferred when the changes are proportional to the level of the series. 

  \subsubsection{Additive Method}
  \begin{align*}
    \text{Level equation}   &&\ell_{t} &= \alpha(y_{t} - s_{t-m}) + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
    \text{Trend equation}   &&b_{t} &= \beta(\ell_{t} - \ell_{t-1}) + (1 - \beta)b_{t-1}\\
    \text{Seasonal equation}   &&s_{t} &= \gamma (y_{t}-\ell_{t-1}-b_{t-1}) + (1-\gamma)s_{t-m}\\
    \text{Forecast equation}   && \hat{y}_{t+h} &= \ell_{t} + hb_{t} + s_{t+h-m(k+1)},
  \end{align*}
  where $k$ is defined as $\lfloor\frac{h-1}{m}\rfloor$. This guarantees that that every forecast uses the seasonal component from the most recently observed season. 

  \subsubsection{Multiplicative Method}
  \begin{align*}
    \text{Level equation}   &&\ell_{t} &= \alpha \frac{y_{t}}{s_{t-m}} + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
    \text{Trend equation}   &&b_{t} &= \beta(\ell_{t}-\ell_{t-1}) + (1 - \beta)b_{t-1}\\
    \text{Seasonal equation}   &&s_{t} &= \gamma \frac{y_{t}}{(\ell_{t-1} + b_{t-1})} + (1 - \gamma)s_{t-m}\\
    \text{Forecast equation}   &&\hat{y}_{t+h} &= (\ell_{t} + hb_{t})s_{t+h-m(k+1)},
  \end{align*}

  \subsubsection{Initial Values for Trend and Seasonal Components}
  Similar to the non-seasonal methods, values can be estimated for the initial trend and initial seasonal components to speed up the optimisation process.

  \subsubsection*{Initial Trend}
  For Holt Linear, we simply used the first two data points for the initial trend. Since we have seasonal data, we can make a better estimate by taking the average trend between the first two seasons:
  \begin{equation*}
    b_0 = \frac{1}{m}\left(\frac{y_{m+1}-y_1}{m} + \frac{y_{m+2}-y_2}{m} + ... + \frac{y_{m+m}-y_m}{m}\right)
  \end{equation*}

  \subsubsection*{Initial Seasonal Components}
  Calculating the initial seasonal components is done in a relatively similar manner. First we compute the averages $A_n$ for each of the $N$ years,
  \begin{equation*}
    A_n = \frac{\sum_{i=1}^m{y_i}}{m}, \qquad n=1,2,...,N.
  \end{equation*}

  The next step depends on whether the additive or multiplicative method is used. For additive, we sum up the differences between the time associated data points in each season with their respective yearly average and finally divide by $N$,
  \begin{align*}
    s_1 &= \frac{1}{N}\left[(y_1-A_1) + (y_{m+1}-A_2) + ... + (y_{m(N-1)+1}-A_N)\right]\\
    s_2 &= \frac{1}{N}\left[(y_2-A_1) +( y_{m+2}-A_2) + ... + (y_{m(N-1)+2}-A_N)\right]\\
    &\vdots
  \end{align*}
  For multiplicative, instead of taking the difference, we divide the data point by the respective average, i.e. $\frac{y_1}{A_1}$.

  \subsubsection{Example}
  \begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=\linewidth]{holt_winters_add_fit.png}
       \caption{Additive fit}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=\linewidth]{holt_winters_mult_fit.png}
      \caption{Multiplicative fit}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=\linewidth]{holt_winters_add_forecast.png}
       \caption{Additive forecast}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=\linewidth]{holt_winters_mult_forecast.png}
      \caption{Multiplicative forecast}
    \end{subfigure}
    \begin{subfigure}[b]{0.75\linewidth}
      \includegraphics[width=\linewidth]{holt_winters_combined_forecast.png}
      \caption{Comparitive forecast}
    \end{subfigure}
    \caption{Fits and forecasts for Holt-Winters' method}
  \end{figure}

  Looking at the comparitive forecast, the differences between additive and multiplicative become clear. The peaks and dips for each season become larger as time goes on whereas it stays constant in the additive case.

  \newpage
  \subsection{Seasonal ARIMA}
  If non-seasonal ARIMA wasn't complicated enough, there exists a seasonal extension which introduces 3 new parameters. A seasonal ARIMA model has two components, non-seasonal and seasonal, which are both structurally the same. Each have their own AR, MA and order of differencing parameters. Combined with non-seasonal differencing, taking a seasonal difference can in some instances help make the data more stationary. Seasonal differencing is simply $y_t' = y_t - y_{t-m}$. 
  
  Seasonal ARIMA models can be written as 
  \begin{equation*}
    ARIMA(p,d,q)\times(P,D,Q)_m
  \end{equation*}
  where $P$, $D$, and $Q$ are the seasonal AR (SAR) terms, seasonal differencing order, and seasonal MA (SMA) terms respectively with $m$ being the period.

  Using backshift notation, the entire equation can be written as:
  \begin{equation*}
    \begin{split}
      (1 - \phi_1B - ... - \phi_pB^p)(1 - \Phi_1B - ... \Phi_PB^{mP})(1 - B)^d(1 - B)^D y_t = \\
      \mu + (1 + \theta_1B + ... \theta_qB^q)(1 + \Theta_1B + ... \Theta_QB^{mQ})\varepsilon_t.
    \end{split}
  \end{equation*}
  In practice, picking the right values for the parameters is very dataset specific. Similarly to ARIMA, the same auto\_arima packages can be used to optimise the parameters for SARIMA models. Otherwise, looking at the ACF and PACF plots also provide a good indicator of what values to choose.

  \begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{SARIMA_111_011_12.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{SARIMA_111_011_12_forecast.png}
    \end{subfigure}
    \caption{SARIMA$(1,1,1)\times(0,1,1)_{12}$ fit and forecasts}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{SARIMA_200_011_12.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{SARIMA_200_011_12_forecast.png}
    \end{subfigure}
    \caption{SARIMA$(2,0,0)\times(0,1,1)_{12}$ fit and forecasts}
  \end{figure}

  \section{Conclusion}
  We covered two of the most popular time series forecasting methods in exponential smoothing and ARIMA models which use vastly different methods. As previously mentioned, accurate forecasting is practically impossible, especially when you are only given a time and value. Recurrent and convolutional neural networks have been employed to deal with extra factors but they require a lot of computational power to operate. 

  \section{Reflection}
  I came across this area during my previous internship where I simply just used the built in packages without really knowing how any of it worked. The fact that the major project was very open ended got me curious and motivated my choice of this topic. During my initial research, I found out how few manual implementations of these models were, especially ARIMA. They all just used the same external libraries with very little explanation. Slowly, after reading more scholarly papers and textbooks, I was able to get a solid fundamental understanding of the mathematics and processes which drive these models and started on the implementation. Throughout the implementation stage, I got caught up trying to get the same results as the external libraries but no matter how many different approaches I tried, I could never fully accomplish it and eventually settled that I was "close enough". Overall, this was a fun project to work on and could be useful to me sometime in the future.

  \section{Resources}
  \url{https://www.probabilitycourse.com/chapter10/10_1_4_stationary_processes.php}
  \url{https://www.statology.org/detrend-data/}

  bibliography\cite{articleFactCheck}
  \printbibliography
  % \end{flushleft}
\end{document}