\documentclass{article}

\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{float}
\usepackage{dirtytalk}

\usepackage[
backend=biber,
style=numeric,
sorting=none
]{biblatex}
\addbibresource{mybib.bib}

\title{Time Series Analysis and Forecasting}
\author{Matthew Juan z5259434}
\date{}
\begin{document}
  \maketitle
  % \begin{flushleft}
  \newpage
  \doublespacing
  \tableofcontents
  \singlespacing
  \newpage
  \section{Introduction to Time Series'}
  Time series are found all over the place and are extremely useful
  to analyse and more importantly, forecast. There are many situations where
  you want to predict past your last data point.
  Use cases include:
  \begin{itemize}
    \item Traders predicting the stock market so they know when to buy or sell
    \item Data centre operators identifying when a critical system may go down or is acting irregularly
    \item Retail store managers knowing how to maximise what stock they order to maximise profit and minimise items that won't sell
  \end{itemize}
  Unfortunatly, just like humans, it is impossible to predict the future.
  However, by analysing the time series, it is possible to formulate
  statistical properites that can be used to give a rough estimate of
  where the data is heading.
  \newpage
  \section{Properties of Time Series Data}
  A time series can fundamentally be thought of as a random variable consisting of a signal and noise. This signal and noise are formed by 3 components which are core to understanding time series', trend, seasonality, and residual. Understanding their role and what they represent as well as some other key definitions are needed before diving in to analysing time series data.
  
  \subsection{Trend}
  The first component is trend. Trend can be viewed as how the data is changing overall. Is the data increasing or decreasing.
  
  \subsection{Seasonality}
  Seasonality refers to a regular occuring pattern that emerges within a given period. E.g peaks or valleys in the data during certain months or hours in the day.
  
  \subsection{Residual}
  Residual, also commonly referred to as errors, are the parts left over after fitting a model to the data. In many time series models, it is defined as the difference between the actual observation and the predicted value.
  
  \subsection{Additive vs Multiplicative}
  The trend and seasonality in a time series can be classified as being additive or multiplicative.
  In additive time series', the data exhibits a general upwards or downwards trend but does so at a constant rate. Peaks and valleys in your data are roughly the same size.
  In contrast, multiplicative time series data exhibits peaks or valleys that are amplified within the seasonal activity. The data becomes exaggerated and the difference between peaks at the start are very different than at the tail.
  \newpage
  \subsection{Stationarity}
  Before going into analysis, the time series needs to be stationary.
  A random variable that forms a time series is classified stationary if it's probabilty distribution is constant throughout the full range of times. 
  Formally, if $T$ is the set of all times in your time series of length $n$, a random process $X$ is stationary if the joint probability distribution at times $t_1,...,t_n \in T$ and $t_1+\Delta,...,t_n+\Delta \in T,\  \Delta \in \mathbb{R}$ are equal. That is
  \begin{equation*}
    F_{X}(x_{t_1},...,x_{t_n}) = F_{X}(x_{{t_1}+\Delta},...,x_{{t_n}+\Delta})\ \forall x_{t_1},...,x_{t_n} \in T
  \end{equation*}
  Intuitively, this means a stationary time series has constant propertues such as mean, variance, covariance, etc. Below shows the differences between stationary and non-stationary processes.
  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{stationary_time_series.png}
    \caption{}
  \end{figure}

  \subsubsection{How to test for stationarity}
    To determine whether a time series is stationary or not, the Augmented Dickey Fuller(ADF) test can be performed. ADF performs a hypothesis test and upon observing p-value $p < \alpha$ where $\alpha$ is commonly 0.05, the hypothesis is rejected and the data is considered to be stationary.

  \subsubsection{Methods of making a time series stationary}
    If you have a non-stationary time series, there are a few ways to make it stationary. Let's use the Shampoo Sales dataset which describes the monthly number of sales of shampoo over a 3 year period.
    The data here has a distinct multiplicative increasing trend and is clearly non stationary. Using the ADF test, the p-value comes out to $1>0.05$ so there is weak evidence against the null hypothesis so the data is non-stationary.
    \begin{figure}[H]
      \includegraphics[width=\linewidth]{data_both.png}
      \caption{}
    \end{figure}
    The most common is differencing. Differencing involves creating a new dataset where the values at time $t$ is equal to the difference between the original value at time $t$ and original value at $t-1$. So if $Y$ is the new dataset and $y$ is the original data, then
    \begin{equation*}
      Y_t = y_t - y_{t-1}.
    \end{equation*}
    Sometimes the differenced data will not make the data stationary enough and it may be necessary to difference the data again known as 2nd order differencing. This process is applicable to higher orders
    Note, the 2nd order difference is not $Y_t = y_t - y_{t-2}$ but it is the first differnce of the first difference so it comes out to
    \begin{equation*}
      Y_t = (y_t - y_{t-1}) - (y_{t-1} - y_{t-2}).
    \end{equation*}
    The p-value from the ADF test is about $1.8 \times 10^{-10} < 0.05$ so the data is now stationary.
    \begin{figure}[H]
      \centering
      \includegraphics[width=\linewidth]{diff_both.png}
      \caption{}
    \end{figure}
    Another common technique is to detrend by model fitting. You simply fit a regression model, such as linear or exponential, and create a new dataset where each value at time $t$ is equal to the difference between the original value and the predicted value.
    When fitted with a 2nd order polynomial regression model, the resulting differences produced a p-value of about $3.4 \times 10^{-10}< 0.05$ so again, the data is now stationary.
    \begin{equation*}
      Y_t = y_t - pred_t.
    \end{equation*}
    Again, the data is much more stationary than the original.
    \begin{figure}[H]
      \centering
      \includegraphics[width=\linewidth]{reg_both.png}
      \caption{}
    \end{figure}
    There are various other techniques that can be performed and combined with one another to make your data more stationary.
    If your data has a seasonal pattern to it, seasonal differencing can be performed where you take the difference between $t$ and $t-\rho$ where $\rho$ is the period of your data.
    Non-linear transformations your dataset such as logging can also prove effective at making data stationary.
  
  \newpage
  
  \section{Non-Seasonal Forecasting Models}
    There are many types of forecasting models but this paper will look two types. ARIMA (Auto-Regressive Integrated Moving Average) and Exponential Smoothing models. For these non-seasonal models, we will use the Shampoo Sales dataset shown previously.

  \subsection{Exponential Smoothing}
    As the name suggests, Exponential Smoothing models are essentially weighted averages of past observations which decay exponentially over time. This type allows for a reliable and quick way to forecast for a wide array of time series'. This section of the paper will go into an implementation of simple exponential smoothing, Holt's linear trend method and the damped trend method.

  \subsubsection{Simple Exponential Smoothing}
  This method of forecasting is extremely simple and is useful when there is no clear trend or seasonal pattern in the data. As mentioned, this method uses a weighted average where the weights decrease exponentially the further in the past the associated observation is. Therefore, for a time series with $T$ datapoints, we can define such an forecast equation as:
  \begin{equation*}
    \hat{y}_{T+1} = \alpha y_T + \alpha(1-\alpha)y_{T-1} + \alpha(1-\alpha)^2y_{T-2} + \alpha(1-\alpha)^3y_{T-3} + ...,
  \end{equation*}
  where $0 \le \alpha \le 1$ is the smoothing parameter and $\hat{y}_{T}$ is the forecasted value at time $T+1$. This equation can be simplified by using the forecasted value at time $T$ to predict the value at time $T+1$ resulting in
  \begin{equation*}
    \hat{y}_{T+1} = \alpha y_T + (1-\alpha)\hat{y}_{T}.
  \end{equation*}

  This method can also be expressed in component form. The only component with single exponential smoothing is the level, $l_t$. In component form, simple exponential smoothing is shown by:
  \begin{align*}
    & \text{Level equation} & \ell_t &= \alpha y_t + (1-\alpha)(\ell_{t-1}), \qquad 0 \le \alpha \le 1  \\
    & \text{Forecast equation} & \hat{y}_{t+h} &= \ell_t \\
  \end{align*}
  The forecasting equation produces a "flat" forecast equal to the last level component since setting $t=T$, we get
  \begin{equation*}
    \hat{y}_{T+h} = \ell_T, \qquad h=1,2,....
  \end{equation*}
  
  Let's see how this is applied to the shampoo dataset. Firstly, we need to estimate values for $\alpha$ as well as the initial level $\ell_0$. By minimising the SSE (sum of squared errors), the estimated parameters are $\alpha=0.40$ and $\ell_0=202.78$. The table below shows the calculations using these parameters.

  \begin{center}
    \begin{tabular}{||c c c c||} 
     \hline
     Time($t$) & Observation($y_t$) & Level($\ell_t$) & Forecast($\hat{y}_t$) \\ [0.5ex] 
     \hline
     0 &       & 202.78 &  \\ 
     \hline
     1 & 266.0 & 228.03 & 202.78 \\
     \hline
     2 & 145.9 & 195.23 & 228.03 \\
     \hline
     3 & 183.1 & 190.38 & 195.23 \\
     \hline
     4 & 119.3 & 161.99 & 190.38 \\
     \hline
     \vdots & \vdots & \vdots & \vdots \\
     \hline
     33 & 682.0 & 541.92 & 448.78 \\
     \hline
     34 & 475.3 & 515.32 & 541.92 \\
     \hline
     35 & 581.3 & 541.67 & 515.32 \\
     \hline
     36 & 646.9 & 583.70 & 541.67 \\
     \hline
     $h$ &  &  & $\hat{y}_{T+h}$ \\
     \hline
     1 &  &  & 583.70 \\
     \hline
     2 &  &  & 583.70 \\
     \hline
     3 &  &  & 583.70 \\
     \hline
     4 &  &  & 583.70 \\
     \hline
     5 &  &  & 583.70 \\
     \hline
    \end{tabular}
  \end{center}

  \begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{simple_exp_fit.png}
      \caption{Fitted values}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{simple_exp_forecast.png}
      \caption{Forecasted values}
    \end{subfigure}
    \caption{Fitted and forecasted values using simple exponential smoothing}
  \end{figure}

  \subsubsection{Trend Methods}
  This section will show Holt's linear trend method and Gardner \& McKenzie's damped trend method which extend simple exponential smoothing by incorporating the trend component.

  \subsubsection*{Holt's Linear Trend Method}
  This method now has three equations, a level smoothing equation, a trend smoothing equation, and a forecast equation:
  \begin{align*}
    \text{Forecast equation} && \hat{y}_{t+h|t} &= \ell_{t} + hb_{t} \\
    \text{Level equation}   && \ell_{t} &= \alpha y_{t} + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
    \text{Trend equation}   && b_{t}    &= \beta(\ell_{t} - \ell_{t-1}) + (1 -\beta)b_{t-1},
  \end{align*}
  where $0 \le \alpha, \beta \le 1$. By including the trend, forecasts are no longer flat but a linear function of $h$. As such, the forecasted value at time $T+h$ is equal to the last level plus $h$ multiplied by the last calculated trend value.

  There are now two smoothing parameters $\alpha$ and $\beta$ as well as two initial states $\ell_0$ and $b_0$ which are estimated to $\alpha=0.49, \beta=0.27, \ell_0=262.93, b_0=-18.01$.

  \begin{center}
    \begin{tabular}{||c c c c c||} 
     \hline
     Time($t$) & Observation($y_t$) & Level($\ell_t$) & Slope($b_t$) & Forecast($\hat{y}_t$) \\ [0.5ex] 
     \hline
     0 &       & 262.93 & -18.01 &  \\ 
     \hline
     1 & 266.0 & 255.19 & -15.23 & 244.92 \\
     \hline
     2 & 145.9 & 194.14 & -27.65 & 244.92 \\
     \hline
     3 & 183.1 & 174.58 & -25.46 & 239.96 \\
     \hline
     4 & 119.3 & 134.60 & -29.40 & 166.489 \\
     \hline
     \vdots & \vdots & \vdots & \vdots & \vdots \\
     \hline
     33 & 682.0 & 583.60 & 39.51 & 540.92 \\
     \hline
     34 & 475.3 & 551.11 & 19.99 & 490.15 \\
     \hline
     35 & 581.3 & 576.07 & 21.33 & 623.11 \\
     \hline
     36 & 646.9 & 621.51 & 27.87 & 571.10 \\
     \hline
     $h$ &  &  &  & $\hat{y}_{T+h}$ \\
     \hline
     1 &  &  &  & 649.38 \\
     \hline
     2 &  &  &  & 677.25 \\
     \hline
     3 &  &  &  & 705.12 \\
     \hline
     4 &  &  &  & 732.99 \\
     \hline
     5 &  &  &  & 760.86 \\
     \hline
    \end{tabular}
  \end{center}

  \begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{holt_linear_fit.png}
      \caption{Fitted values}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{holt_linear_forecast.png}
      \caption{Forecasted values $\phi=0.9$}
    \end{subfigure}
    \caption{Fitted and forecasted values using Holt linear trend method}
  \end{figure}

  \subsubsection*{Damped Trend Method}
  Holt's Linear method show a constant increasing or decreasing trend which extends indefinitely into the future. However, in reality this method can be quite inaccurate for extended forecasts. Gardner \& McKenzie introduced a dampening parameter $\phi$, where $0 \le \phi \le 1$. This parameter dampens the trend to forecast a flat line after some time.

  The component form of this method is:
  \begin{align*}
    \text{Forecast equation}&& \hat{y}_{t+h|t} &= \ell_{t} + (\phi+\phi^2+...+\phi^h)b_{t} \\
    \text{Level equation}   && \ell_{t} &= \alpha y_{t} + (1 - \alpha)(\ell_{t-1} + \phi b_{t-1})\\
    \text{Trend equation}   && b_{t}    &= \beta(\ell_{t} - \ell_{t-1}) + (1 -\beta)\phi b_{t-1}.
  \end{align*}
  When $\phi=1$, this method produces Holt's linear method.

  \begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=\linewidth]{damped_forecast.png}
    \caption{Forecasts for different values of $\phi$}
  \end{figure}

  \subsection{ARIMA}
    ARIMA is an extension of the ARMA or Box-Jenkin model popularised by George E. P. Box and Gwilym Jenkins in 1970. The main difference is that ARIMA simply allows for an extra parameter $d$ which defines how much differencing should be done before fitting the model, whereas ARMA assumes the data is stationary already. 
    ARIMA is actually a combination of three components, an autoregressive model (AR), a moving averages model (MA), and an integrated (I) part denoting the previously mentioned differencing. We will dive into what makes up AR and MA models and how ARIMA is able to effectively combine the two.
    As previously discussed, a random variable in the form of a time series can be viewed as being a combination of signal and noise. The goal of ARIMA is to filter the noise and extrapolate the signal.
  
  \subsubsection{Backshift Notation}
  Before we jump into defining what an ARIMA model is, there is a useful backshift operator $B$ denoting the time series lags,
  \begin{equation*}
    By_t = y_{t-1}.
  \end{equation*}
  $B$ effectively shifts that data back one time period. So naturally, two applications of $B$ shifts the data back two time periods. 
  \begin{align*}
    B(By_t) &= B^2y_t \\
    &= y_{t-2}.
  \end{align*}
  If we describe how differencing works using this notation it comes out to
  \begin{align*}
    Y_t &= y_t - y_{t-1} \\
    &= y_t - By_t \\
    &= (1 - B)y_t.
  \end{align*}
  You will start to see a pattern when you take higher order differences. For second order,
  \begin{align*}
    Y_t &= (y_t - y_{t-1}) - (y_{t-1} - y_{t-2}) \\
    &= (y_t - 2By_t + B^2y_t) \\
    &= (1 - B)^2y_t.
  \end{align*}
  In general, the $d$'th order of differencing is defined as 
  \begin{equation*}
    Y_t = (1 - B)^dy_t.
  \end{equation*}
  This notation is great as we can use $B$ is an algebraic manner which greatly simplifies some upcoming formulas.

  \subsubsection{Autoregressive Models}
    The term \emph{autoregressive} defines that the model uses a linear combination of previous values of a variable and learned predictors. This differs from a typical regression model which predicts based on a linear combination of the input features and learned predictors.
    Autoregressive models has one parameter $p$ which defines the order of the model. This parameter defines how many previous values influence the one being predicted. 
    An AR$(1)$ model is simply defines a model in which the predicted value is equal to some linear combination of the value at the previous time step
    \begin{equation*}
      y_t = \mu + \phi_1 y_{t-1} + \varepsilon_t,
    \end{equation*}
    where $\mu$ is the average period to period change.
    An AR$(2)$ model uses the previous two timesteps as so on.
    Thus, an AR$(p)$ model can be generalised to
    \begin{gather*}
      y_t = \mu + \phi_1 y_{t-1} + \phi_2 y_{t-2} + ... + \phi_p y_{t-p}\\
      \text{or in equivalent backshift notation}\\
      (1 - \phi_1B - \phi_2B^2 - ... - \phi_pB^p)y_t = \mu.
    \end{gather*}
    We can see that we are using previous known values as the features to predict the future.

    Let's see how this works for the first differenced data. Firstly, the lags are calculated and then we estimate parameters using these lags and our original time series values. We simply do a dot product between the coefficients and add it to our value for $\mu$ to retrieve our predictions. 
    
    This is the resulting predictions of a few AR$(p)$ models.

    \begin{figure}[H]
      \centering
      \captionsetup{justification=centering}
      \begin{subfigure}[b]{0.49\linewidth}
        \includegraphics[width=\linewidth]{AR_1.png}
        \caption{AR(1)}
      \end{subfigure}
      \begin{subfigure}[b]{0.49\linewidth}
        \includegraphics[width=\linewidth]{AR_2.png}
        \caption{AR(2)}
      \end{subfigure}
      \caption{Two types of AR models. AR(1) with estimated $\phi_1 = -0.70868438$. AR(2) with estimated $\phi_1 = -0.9237595, \phi_2 =   -0.26627682$.}
    \end{figure}

    Observe that there are subtle changes between the two. If we kept on increasing the AR term, our model would fit this data extremely well. In machine learning, this is commonly known as overfitting and future forecasts can be very inaccurate.

  \subsubsection{Moving Average Models}
    In a sort of similar manner, Moving Average (MA) models use the errors of past forecasts to predict. An MA model instead has parameter $q$ so an MA$(q)$ model can be written similarly as
    \begin{gather*}
      y_t = \mu + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + ... + \theta_q \varepsilon_{t-q}\\
      \text{or in equivalent backshift notation}\\
      y_t = \mu + (\theta_1B + \theta_2B^2 + ... + \theta_qB^q)\varepsilon_t.
    \end{gather*}
    If your data is properly stationary, the error terms produced should form white noise in the form of a normal distribution with $0$ mean and $\theta^2$ variance.

    \begin{figure}[H]
      \centering
      \captionsetup{justification=centering}
      \begin{subfigure}[b]{0.49\linewidth}
        \includegraphics[width=\linewidth]{residuals.png}
        \caption{MA(1)}
      \end{subfigure}
      \begin{subfigure}[b]{0.49\linewidth}
        \includegraphics[width=\linewidth]{residuals2.png}
        \caption{MA(2)}
      \end{subfigure}
      \caption{Error terms for MA(1) with estimated $\theta_1 = -0.58305195$ and MA(2) with estimated $\theta_1 = -0.71206191, \theta_2 =  0.28793809$. Both generally look normally distributed with 0 mean.}
    \end{figure}

  \subsubsection{Putting It All Together}
  When we combine differencing, an AR, and an MA model, we obtain an ARIMA model. It's general equation is as follows
  \begin{gather*}
    y_t' = \mu + \phi_1 y_{t-1}' + ... + \phi_p y_{t-p}' + \theta_1 \varepsilon_{t-1}' + ... + \theta_q \varepsilon_{t-q}'\\
    \text{or in equivalent backshift notation}\\
    (1 - \phi_1B - ... - \phi_pB^p)(1-B)^dy_t = \mu + (\theta_1B + ... + \theta_qB^q)\varepsilon_t.
  \end{gather*}
  where $y_t'$ is the differenced data. Note that the backshift notation already takes into account the order of differencing $d$ in its equation.

  An ARIMA model has 3 parameters and can be expressed as ARIMA$(p,d,q)$.
  \begin{itemize}
    \item $p$ is order of autoregressive component
    \item $d$ is the order of differencing to be performed on the data
    \item $q$ is the order of moving averages component
  \end{itemize}
  This generalised model is great since we don't have to difference the data ourselves and allows for the combination of AR$(p)$ and MA$(q)$ models.  

  Running through a list of parameters, we can see effects of different values for the parameters.

  \begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{ARIMA_010.png}
      \caption{ARIMA$(0, 1, 0)$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{ARIMA_111.png}
      \caption{ARIMA$(1, 1, 1)$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{ARIMA_110.png}
      \caption{ARIMA$(1, 1, 0)$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{ARIMA_011.png}
      \caption{ARIMA$(0, 1, 1)$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{ARIMA_211.png}
      \caption{ARIMA$(2, 1, 1)$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{ARIMA_112.png}
      \caption{ARIMA$(1, 1, 2)$}
    \end{subfigure}
    \caption{Fits for different values of $p$ and $q$}
  \end{figure}

  \subsubsection{Model Scoring}
  There are a few ways to score how well ARIMA models fit and predict. The most popular way is using Akaine's Information Criterion (AIC). It can be written as 

  \begin{equation*}
    AIC = -2log(L) + 2(p + q + k + 1),
  \end{equation*}
  where $L$ is the likelihood of the data, $k = 1$ if $\mu \ne 0$ and $k = 0$ if $\mu = 0$. The general goal is to pick the model which minimises this value. Other values such as the Bayesian Information Criterion (BIC) and root mean squared error are also good metrics to measure and consider for your model.

  Looking at the previous models, we can compare their AIC:
  \begin{center}
    \begin{tabular}{||c c||} 
     \hline
     Model & AIC \\ [0.5ex] 
     \hline\hline
     ARIMA$(0,1,0)$ & 447.8405 \\ 
     \hline
     ARIMA$(1,1,1)$ & 430.1539 \\
     \hline
     ARIMA$(1,1,0)$ & 429.1197 \\ 
     \hline
     ARIMA$(0,1,1)$ & 433.8894 \\
     \hline
     ARIMA$(2,1,1)$ & 430.3742 \\
     \hline
     ARIMA$(1,1,2)$ & 431.4727 \\
     \hline
    \end{tabular}
  \end{center}
  Here, it looks like our ARIMA$(1,1,0)$ and ARIMA$(1,1,1)$ models had the best AIC scores.

  \subsubsection{Coefficient Estimation}
  Estimating the coefficients for an ARIMA model is often a tricky task as there needs to be a balance between having a good fit for the data while not overfitting. For my implementation, both the AR and MA terms were calculated similarly using the Constrained Optimization BY Linear Approximation (COBYLA) algorithm which optimises based on the sum of squared errors. To avoid overfitting the data, I set constraints on the coefficients namely,
  \begin{itemize}
    \item $-1 \leq \phi_p \leq 1$
    \item $\phi_i + \phi_{i+1} \leq 1 \quad \text{for} \ i=1,2,...,p-1$
    \item $\phi_{i+1} - \phi_i \leq 1 \quad \text{for} \ i=1,2,...,p-1$
  \end{itemize}
  
  The same constraints are put for $\theta_1,...,\theta_q$. In practice, ARIMA packages in Python and R use Maximum Likelihood Estimation to estimate the coefficients which often invoves much more complicated computations. There are also more complicated constraints, especially as the values of $p$ and $q$ increase, which these packages handle that account for the invertability and stationarity constraints.

  \newpage
  \subsubsection{Order Choosing}
  Up until now, we have been manually setting the parameters $p$, $d$, and $q$ seemingly by observing which model fits the best or which one has the best AIC score. However, there are systematic methods of narrowing down the search space. This is where we look at the autocorrelation function (ACF) plots and partial autocorrelation (PACF) plots to help determine our optimal parameters. Autocorrelation is a way of measuring the similarity between values in a time series and their past. An autocorrelation of $+1$ means that there is a perfect positive correlation while $-1$ represents a perfect negative correlation. Partial autocorrelation is similar to autocorrelation but slightly more complicated to compute. PACF again measures similarity between $y_t$ and $y_{t-\Delta}$ but adjusts for the presence of the other terms of short lag ($y_{t-1}, y_{t-2},...,y_{t-\Delta-1}$).

  \begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{\linewidth}
      \includegraphics[height=6.2cm, width=\linewidth]{acf_pacf_data.png}
      \caption{ACF and PACF of undifferenced data}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
      \includegraphics[height=6.2cm, width=\linewidth]{acf_pacf_first.png}
      \caption{ACF and PACF of order 1 differenced data}
    \end{subfigure}
    % \caption{ACF and PACF plots of our non-differenced and once differenced data}
  \end{figure}
  

  In a perfect scenario, we would want no correlation between the series and the lags of itself. Of course, this is almost impossible to achieve so the blue region highlights where we wish our spikes to lie. 

  Firstly, we have to choose the order of differencing. It is not sufficient to say that because a once differenced series is stationary then that should be the order. Higher order differencing may produce better results. This is where we look at the autocorrelation function (ACF) plot to help pick the value for $d$. 

  Duke University professor, Dr Robert Nau presents a few rules for interpreting these graphs and how to estimate the optimal value for $d$.
  \newline
  \newline
  \emph{\say{Rule 1: If the series has positive autocorrelations out to a high number of lags (say, 10 or more), then it probably needs a higher order of differencing.}\\
  \say{Rule 2: If the lag-1 autocorrelation is zero or negative, or the autocorrelations are all small and patternless, then the series does not need a higher order of differencing. If the lag-1 autocorrelation is -0.5 or more negative, the series may be overdifferenced.  BEWARE OF OVERDIFFERENCING.}\\
  Dr Robert Nau, Statistical Forecasting}
  \newline

  Looking at the ACF plot for our undifferenced data, it mostly stays positive until around lag 11 so by rule 1, we should difference at least once. We can see that rule 1 no longer applies and rule 2 is applied. Observe that the lag-1 autocorrelation does go negative so a good choice would be $d=1$. Note that the value is less than $-0.5$ which the rule warns us of overdifferencing. However, previously we looked at the ADF tests for differenced and non-differenced and it was clear that differencing was required so $d=1$ is a good choice.

  Using this method can be preferred over the trial and error method with the ADF test if we only care about differencing our data.\\
  By looking at the ACF and PACF plots of a stationary series, we can generally identify the numbers of AR and/or MA terms that are needed. Dr Nau has identified more rules in identifying these values.
  \newline
  \newline
  \emph{\say{Rule 6: If the partial autocorrelation function (PACF) of the differenced series displays a sharp cutoff and/or the lag-1 autocorrelation is positive, then consider adding one or more AR terms to the model. The lag which the PACF cuts off is the indicated number of AR terms.}\\
  \say{Rule 7: If the autocorrelation function (ACF) of the differenced series displays a sharp cutoff and/or the lag-1 autocorrelation is negative, then consider adding an MA term to the model. The lag which the ACF cuts off is the indicated number of MA terms.}}
  \newline

  Rule 6 helps us determine the AR terms. From the PACF plot, at lag-1 there is a steep drop into the negatives then settles into the blue region. Hence, we probably need 1 AR term. Similarly by rule 7, we should pick 1 MA term.

  This gives us an ARIMA$(1,1,1)$ which we have seen before. This gives us an ARIMA$(1,1,1)$ model which we have seen before. In Python there is a package known as pmd\_arima which has a nice function called auto\_arima which computes the best parameters given some data. In this case, it gives back a parameter list of $(1,1,2)$ so our estimation was pretty close and shows that these rules are not absolute.
  

  \subsubsection{Forecasting}
  So far, we have shown how to fit an ARIMA model on a well-defined time series. But we have not gone into how it forecasts unseen data. It is quite simple and involves 3 steps:
  \begin{enumerate}
    \item Expand the model's ARIMA backshift equation and group by the powers of B
    \item Convert to equation with only $y_t$'s and move all terms that isn't $y_t$ to the right side of the equation
    \item Replace $t$ with $T+1$ is the number of future time steps and update time series with predicted value
  \end{enumerate}
  Let's try this with an example using an ARIMA(2,1,2) model as an example. In backshift notation, the model is written as
  \begin{equation*}
    (1 - \phi_1B - \phi_2B^2)(1-B)y_t = \mu + (\theta_1B + \theta_2B^2)\varepsilon_t.
  \end{equation*}
  Using step 1, we expand and group:
  \begin{align*}
    (1 - B - \phi_1B + \phi_1B^2 - \phi_2B^2 + \phi_2B^3)y_t &= \mu + (\theta_1B + \theta_2B^2)\varepsilon_t\\
    [1 - (1 + \phi_1)B - (\phi_2 - \phi_1)B^2 + \phi_2B^3]y_t &= \mu + (\theta_1B + \theta_2B^2)\varepsilon_t.
  \end{align*}
  With second step, we convert giving
  \begin{equation*}
    y_t - (1 + \phi_1)y_{t-1} - (\phi_2 - \phi_1)y_{t-2} + \phi_2y_{t-3} = \mu + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2},\\
  \end{equation*}
  then moving terms
  \begin{equation*}
    y_t = \mu + (1 + \phi_1)y_{t-1} + (\phi_2 - \phi_1)y_{t-2} - \phi_2y_{t-3} + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2}.
  \end{equation*}
  Third step is to replace $t$ with $T+1$:
  \begin{equation*}
    y_{T+1} = \mu + (1 + \phi_1)y_{T} + (\phi_2 - \phi_1)y_{T-1} - \phi_2y_{T-2} + \theta_1\varepsilon_{T-1} + \theta_2\varepsilon_{T-2}.
  \end{equation*}
  To predict further into the future say at time $T+2$, then just repeat step 3 which produces
  \begin{equation*}
    y_{T+2} = \mu + (1 + \phi_1)y_{T+1} + (\phi_2 - \phi_1)y_{T} - \phi_2y_{T-1} + \theta_2\varepsilon_{T-1}.
  \end{equation*}
  Observe that there should be a $\theta_1\varepsilon_{T}$ term but since we set the time series value at $T$ to be equal to the predicted value, this error term evaluates to 0. We are essentially taking our predicted value as the truth which is why all the error terms eventually become 0. Hence, if you predict $S > q$ future time steps, all MA terms disappear. 

  \begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{forecast_ARIMA_010.png}
      \caption{ARIMA$(0, 1, 0)$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{forecast_ARIMA_111.png}
      \caption{ARIMA$(1, 1, 1)$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{forecast_ARIMA_110.png}
      \caption{ARIMA$(1, 1, 0)$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{forecast_ARIMA_011.png}
      \caption{ARIMA$(0, 1, 1)$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{forecast_ARIMA_211.png}
      \caption{ARIMA$(2, 1, 1)$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{forecast_ARIMA_112.png}
      \caption{ARIMA$(1, 1, 2)$}
    \end{subfigure}
    \caption{Forecasting for 15 future time steps from different ARIMA models. (c) and (d) are equivalent to an AR(1) and MA(1) with 1 order of differencing.}
  \end{figure}
  
  \newpage

  \section{Seasonal models}
  Many time series data in the real work exhibit some form of seasonality. This includes higher network traffic during peak hours of the day or increased spending during Christmas. As such, there is a defined frequency of seasonality commonly denoted as $m$. For example $m=4$ could represent quarterly data and $m=12$ for monthly data. We will again explore the ARIMA and Exponential Smoothing algorithms and how they were extended to account for seasonality. For these examples, we will use the monthly milk dataset. This dataset is great since there is a clear seasonal pattern and the data is trending upward.
  \begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{\linewidth}
      \includegraphics[width=\linewidth]{milk.png}
    \end{subfigure}
    \caption{Milk dataset}
  \end{figure}

  \newpage
  \subsection{Holt-Winters' Seasonal Method}
  Holt and Winters improved Holt's linear method to capture seasonality. This method includes a level equation, trend equation, seasonal equation, and forecasting equation. It introduces a new smoothing parameter $\gamma$ and $m$ initial seasonal values that have to be estimated. This method comes in two flavours, an additive and multiplicative method. Which one you pick is dependent on your data. The additive method is preferred when seasonal variations and trend change at a constant rate. Peaks and valleys in your data are roughly of the same magnitude. In contrast, multiplicative is preferred when the changes are proportional to the level of the series. 

  \subsubsection{Additive Method}
  \begin{align*}
    \text{Level equation}   &&\ell_{t} &= \alpha(y_{t} - s_{t-m}) + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
    \text{Trend equation}   &&b_{t} &= \beta(\ell_{t} - \ell_{t-1}) + (1 - \beta)b_{t-1}\\
    \text{Seasonal equation}   &&s_{t} &= \gamma (y_{t}-\ell_{t-1}-b_{t-1}) + (1-\gamma)s_{t-m}\\
    \text{Forecast equation}   && \hat{y}_{t+h} &= \ell_{t} + hb_{t} + s_{t+h-m(k+1)},
  \end{align*}
  where $k$ is defined as $\lfloor\frac{h-1}{m}\rfloor$. This guarantees that that every forecast uses the seasonal component from the most recently observed season. 

  \subsubsection{Multiplicative Method}
  \begin{align*}
    \text{Level equation}   &&\ell_{t} &= \alpha \frac{y_{t}}{s_{t-m}} + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
    \text{Trend equation}   &&b_{t} &= \beta(\ell_{t}-\ell_{t-1}) + (1 - \beta)b_{t-1}\\
    \text{Seasonal equation}   &&s_{t} &= \gamma \frac{y_{t}}{(\ell_{t-1} + b_{t-1})} + (1 - \gamma)s_{t-m}\\
    \text{Forecast equation}   &&\hat{y}_{t+h} &= (\ell_{t} + hb_{t})s_{t+h-m(k+1)},
  \end{align*}

  \subsubsection{Initial Values for Trend and Seasonal Components}
  Similar to the non seasonal method, values can be estimated for the initial trend and initial seasonal components to speed up the optimisation process.

  \subsubsection*{Initial Trend}
  For Holt Linear, we simply used the first two data points for the initial trend. Since we have seasonal data, we can make a better estimate by taking the average trend between the first two seasons:
  \begin{equation*}
    b_0 = \frac{1}{m}\left(\frac{y_{m+1}-y_1}{m} + \frac{y_{m+2}-y_2}{m} + ... + \frac{y_{m+m}-y_m}{m}\right)
  \end{equation*}

  \subsubsection*{Initial Seasonal Components}
  Calculating the initial seasonal components is done in a relatively similar manner. First we compute the averages $A_n$ for each of the $N$ years,
  \begin{equation*}
    A_n = \frac{\sum_{i=1}^m{y_i}}{m}, \qquad n=1,2,...,N.
  \end{equation*}

  The next step depends on whether the additive or multiplicative method is used. For additive, we sum up the differences between the time associated data points in each season with their respective yearly average and finally divide by $N$,
  \begin{align*}
    s_1 &= \frac{1}{N}\left[(y_1-A_1) + (y_{m+1}-A_2) + ... + (y_{m(N-1)+1}-A_N)\right]\\
    s_2 &= \frac{1}{N}\left[(y_2-A_1) +( y_{m+2}-A_2) + ... + (y_{m(N-1)+2}-A_N)\right]\\
    &\vdots
  \end{align*}
  For multiplicative, instead of taking the difference, we divide the data point by the respective average, i.e. $\frac{y_1}{A_1}$.

  \begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{holt_winters_add_fit.png}
       \caption{Additive fit}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{holt_winters_mult_fit.png}
      \caption{Multiplicative fit}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{holt_winters_add_forecast.png}
       \caption{Additive forecast}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{holt_winters_mult_forecast.png}
      \caption{Multiplicative forecast}
    \end{subfigure}
    \begin{subfigure}[b]{0.75\linewidth}
      \includegraphics[width=\linewidth]{holt_winters_combined_forecast.png}
      \caption{Comparitive forecast}
    \end{subfigure}
    \caption{Fits and forecasts for Holt-Winters' method}
  \end{figure}

  Looking at the comparitive forecast, the differences between additive and multiplicative become apparent. The difference between the peaks and dips for each season become larger as time goes on whereas it stays constant in the additive case.

  \newpage
  \subsection{Seasonal ARIMA}
  If non-seasonal ARIMA wasn't complicated enough, there exists a seasonal extension which introduces 3 new parameters. A seasonal ARIMA model has two components, non-seasonal and seasonal, which are both structurally the same. Each have their own AR, MA and order of differencing parameters. Combined with non-seasonal differencing, taking a seasonal difference can in some instances help make the data more stationary. Seasonal differencing is simply $y_t' = y_t - y_{t-m}$. Seasonal ARIMA models can be written as 
  \begin{equation*}
    ARIMA(p,d,q)\times(P,D,Q)_m
  \end{equation*}
  where $P$, $D$, and $Q$ are the seasonal AR (SAR) terms, seasonal differencing order, and seasonal MA (SMA) terms respectively with $m$ being the period.

  Using backshift notation, the entire equation can be written as:
  \begin{equation*}
    \begin{split}
      (1 - \phi_1B - ... - \phi_pB^p)(1 - \Phi_1B - ... \Phi_PB^{mP})(1 - B)^d(1 - B)^D y_t = \\
      \mu + (1 + \theta_1B + ... \theta_qB^q)(1 + \Theta_1B + ... \Theta_QB^{mQ})\varepsilon_t.
    \end{split}
  \end{equation*}

  Using the milk data, we pick $m = 12$ since the data is monthly.
  First let's figure out our order of non-seasonal and seasonal orders of differencing. We can first try 1 order of seasonal differencing and looking at the ACF and PACF graphs.
  \begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{\linewidth}
      \includegraphics[width=\linewidth]{acf_pacf_seasonaldiff_data.png}
    \end{subfigure}
    \caption{ACF and PACF for seasonal differenced data}
  \end{figure}

  The slow decrease in the ACF signifies and p-value $\approx 0.1608 >=0.05$ indicates that we need more orders of differencing. This time we will try include a non-seasonal difference.
  \begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{\linewidth}
      \includegraphics[width=\linewidth]{acf_pacf_seasonaldiff_nonseasonaldiff_data.png}
    \end{subfigure}
    \caption{ACF and PACF for seasonal differenced + non seasonal differenced data}
  \end{figure}

  This looks a lot better. With a p-value $\approx 1.8654\times10^{-5}<0.05$ we can be happy with our orders of differencing being $d=1$ and $D=1$. Note, by the equation, the order in which you difference the series, either seasonal or non-seasonal first, does not make a difference to the values of the overall differenced series.
  
  Once again, Dr Nau has 2 rules in picking these parameters:
  \newline
  \emph{\say{Rule 12: If the series has a strong and consistent seasonal pattern, then you should use an order of seasonal differencing--but never use more than one order of seasonal differencing or more than 2 orders of total differencing (seasonal+nonseasonal).}\\
  \say{Rule 13: If the autocorrelation at the seasonal period is positive, consider adding an SAR term to the model. If the autocorrelation at the seasonal period is negative, consider adding an SMA term to the model. Try to avoid mixing SAR and SMA terms in the same model, and avoid using more than one of either kind.}}

  Combining these and the previous rules, it may be wise to pick a\\
  SARIMA$(1,1,1)\times(0,1,1)_{12}$ model which comes out with an AIC $=1068.064$. Running this through the
  auto\_arima function, surprisingly yields a SARIMA$(2,0,0)\times(0,1,1)_{12}$ model with an AIC $=1076.388$. Both models fit the data extremely well and the forecasts are able to replicate the rise and fall pattern.

  \begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{\linewidth}
      \includegraphics[width=\linewidth]{SARIMA_111_011_12.png}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
      \includegraphics[width=\linewidth]{SARIMA_111_011_12_forecast.png}
    \end{subfigure}
    \caption{SARIMA$(1,1,1)\times(0,1,1)_{12}$ fit and forecasts}
  \end{figure}

  \begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{\linewidth}
      \includegraphics[width=\linewidth]{SARIMA_200_011_12.png}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
      \includegraphics[width=\linewidth]{SARIMA_200_011_12_forecast.png}
    \end{subfigure}
    \caption{SARIMA$(2,0,0)\times(0,1,1)_{12}$ fit and forecasts}
  \end{figure}

  \section{Conclusion}

  \section{Resources}
  \url{https://www.probabilitycourse.com/chapter10/10_1_4_stationary_processes.php}
  \url{https://www.statology.org/detrend-data/}

  bibliography\cite{articleFactCheck}
  \printbibliography
  % \end{flushleft}
\end{document}